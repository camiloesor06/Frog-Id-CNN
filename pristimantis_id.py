# -*- coding: utf-8 -*-
"""Pristimantis ID.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Shg0A34VuCP4ZHn62vSK4kFyRDXWfTCE

# Pristimantis Species Identification using CNNs in Farallones National Park

## Author: Camilo Andrés Estupiñan

In this notebook, we address the classification and localization of 12 frog species of the genus Pristimantis from PNN Farallones de Cali. A Convolutional Neural Network (CNN) based on transfer learning (ResNet50) was implemented.
[link text](https://)
Given the limited number of training images per species, data augmentation was applied.

# Libraries
"""

from __future__ import print_function, division

import numpy as np
import pandas as pd
from numpy.typing import NDArray
from functools import reduce
from itertools import islice

import math
from itertools import chain
import copy

import torch
from torch import nn
from torch import Tensor
from torch.optim import Optimizer

import torchvision.models as models
import torch.optim as optim

import torch.nn.functional as F
import torchvision
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils
from torchsummary import summary

# Import albumentations library in order to -use pre-built augmentations
import albumentations as A


from sklearn.model_selection import train_test_split
from multiprocessing import cpu_count

import os
import torch
import os.path as osp
from skimage import io, transform
import matplotlib.pyplot as plt

import matplotlib.patches as patches

import typing as ty
import cv2

plt.ion()   # interactive mode

torch.manual_seed(32)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Using {device}')
test = torch.ones((100, 100)).to(device)
del test
torch.cuda.empty_cache()

"""# Upload the data"""

from google.colab import files
uploaded = files.upload()

import zipfile
import tarfile
import os

# Para un archivo .zip
if os.path.exists("For python.zip"):
    with zipfile.ZipFile("For python.zip", 'r') as zip_ref:
        zip_ref.extractall("/content")

# Para un archivo .tar.gz
if os.path.exists("For python.tar.gz"):
    with tarfile.open("For python.tar.gz", "r:gz") as tar_ref:
        tar_ref.extractall("/content")

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

DATA_DIR = '/content/For python/'
WORK_DIR = '/content/For python/'


BATCH_SIZE = 16

img_dir = osp.join(DATA_DIR, "resized")
h, w, c = 416, 416, 3 # The heigh, width and number of channels of each image

# Here we map each class to an index from 0 to n_classes -

df = pd.read_csv(osp.join(DATA_DIR, "train.csv"))

#df.drop(columns=['class_id','class'], inplace=True)
#df.rename(columns = {'id_tipo':'class_id','Tipo':'class'}, inplace=True)

id2obj = df[['class_id','class']].drop_duplicates().reset_index().reset_index().drop(columns=['index','class_id']).rename(columns={'level_0':'class_id'}).set_index('class_id')['class'].to_dict()
obj2id = df[['class_id','class']].drop_duplicates().reset_index().reset_index().drop(columns=['index','class_id']).rename(columns={'level_0':'class_id'}).set_index('class')['class_id'].to_dict()

df["class_id"] = df["class"].map(obj2id)

df

df = df[df['filename'] != 'Pristimantis_erythropleura_inat_013.jpg'].reset_index(drop=True) # remove a problematic row

from PIL import Image
list_image = list(df.filename)
data_shape = []
data_dim = []
data_w = []
data_h = []

for i in list_image:
    ruta_imagen = osp.join(img_dir, i)
    try:
        imagen = io.imread(ruta_imagen)
        shapes = imagen.shape
        dimen = imagen.ndim
        imagen = Image.open(ruta_imagen)
        w, h = imagen.size
        data_w.append(w)
        data_h.append(h)
        data_shape.append(shapes)
        data_dim.append(dimen)
    except FileNotFoundError:
        print(f"Archivo no encontrado: {ruta_imagen}")
        continue

data_w_h = pd.DataFrame([list_image,data_shape,data_dim,data_w,data_h]).T.rename(columns={0:'filename',1:'shapes',2:'ndim',3:'w',4:'h'})

data_w_h

data_w_h['ndim'].value_counts()

df = df.merge(data_w_h[['filename','w','h']], how='inner', on='filename')

df['w'] = pd.to_numeric(df['w'], errors='coerce')
df['h'] = pd.to_numeric(df['h'], errors='coerce')

df

print("Clases únicas:")
print(df['class'].unique())


num_classes = df['class_id'].nunique()
print("Número total de clases:", num_classes)

# Ver el máximo valor de class_id
print("Máximo valor de class_id:", df['class_id'].max())

# Ver todos los valores únicos
print("Valores únicos en class_id:", sorted(df['class_id'].unique()))

"""# Train and validation split"""

df['xmin'] = np.round(df['xmin']/df['w'],2)
df['xmax'] = np.round(df['xmax']/df['w'],2)
df['ymin'] = np.round(df['ymin']/df['h'],2)
df['ymax'] = np.round(df['ymax']/df['h'],2)

columns_f = ['filename', 'xmin','ymin','xmax','ymax','class_id','class','w','h']
df1 = df[columns_f].copy()

train_df, val_df = train_test_split(
    df1, stratify=df1['class_id'], test_size=0.25
)

print(train_df.shape)
print(val_df.shape)

train_df['class'].value_counts(1) * 100

val_df['class_id'].value_counts(1) * 100

train_df.shape

val_df.shape

"""# Class Balancing

We will repeat this approach 3 times (This worked and ended improving the accuracy)

First run
"""

original_counts = train_df['class_id'].value_counts().sort_index()


augmented_dfs = []
replicated_counts = {}

for class_id, count in original_counts.items():
    class_subset = train_df[train_df['class_id'] == class_id]


    if count < 100:
        num_to_add = 150
    elif 100 <= count <= 250:
        num_to_add = 80
    else:  # count > 250
        num_to_add = 20


    replicas = class_subset.sample(n=num_to_add, replace=True, random_state=42)
    replicated_counts[class_id] = len(replicas)

    full_class_df = pd.concat([class_subset, replicas], ignore_index=True)
    augmented_dfs.append(full_class_df)


train_df = pd.concat(augmented_dfs).sample(frac=1, random_state=42).reset_index(drop=True)


final_counts = train_df['class_id'].value_counts().sort_index()
replicated_series = pd.Series(replicated_counts)

summary_df = pd.DataFrame({
    'original': original_counts,
    'added': replicated_series,
    'final': final_counts
})

print(summary_df)

"""Second run:"""

original_counts = train_df['class_id'].value_counts().sort_index()


augmented_dfs = []
replicated_counts = {}

for class_id, count in original_counts.items():
    class_subset = train_df[train_df['class_id'] == class_id]


    if count < 100:
        num_to_add = 150
    elif 100 <= count <= 250:
        num_to_add = 80
    else:  # count > 250
        num_to_add = 20


    replicas = class_subset.sample(n=num_to_add, replace=True, random_state=42)
    replicated_counts[class_id] = len(replicas)

    full_class_df = pd.concat([class_subset, replicas], ignore_index=True)
    augmented_dfs.append(full_class_df)


train_df = pd.concat(augmented_dfs).sample(frac=1, random_state=42).reset_index(drop=True)


final_counts = train_df['class_id'].value_counts().sort_index()
replicated_series = pd.Series(replicated_counts)

summary_df = pd.DataFrame({
    'original': original_counts,
    'added': replicated_series,
    'final': final_counts
})

print(summary_df)

"""Thrid run:"""

original_counts = train_df['class_id'].value_counts().sort_index()


augmented_dfs = []
replicated_counts = {}

for class_id, count in original_counts.items():
    class_subset = train_df[train_df['class_id'] == class_id]


    if count < 100:
        num_to_add = 150
    elif 100 <= count <= 250:
        num_to_add = 80
    else:  # count > 250
        num_to_add = 20


    replicas = class_subset.sample(n=num_to_add, replace=True, random_state=42)
    replicated_counts[class_id] = len(replicas)

    full_class_df = pd.concat([class_subset, replicas], ignore_index=True)
    augmented_dfs.append(full_class_df)


train_df = pd.concat(augmented_dfs).sample(frac=1, random_state=42).reset_index(drop=True)


final_counts = train_df['class_id'].value_counts().sort_index()
replicated_series = pd.Series(replicated_counts)

summary_df = pd.DataFrame({
    'original': original_counts,
    'added': replicated_series,
    'final': final_counts
})

print(summary_df)

"""We check balancing:"""

print(train_df['class_id'].value_counts(normalize=True))

train_df.shape

"""# Create class"""

transform_func_inp_signature = ty.Dict[str, NDArray[np.float64]]
transform_func_signature = ty.Callable[
    [transform_func_inp_signature],
    transform_func_inp_signature
]

class PristimantisDataset(Dataset):
    """
    Location Pristimantis dataset
    """
    def __init__(
        self,
        df: pd.DataFrame,
        root_dir: str,
        labeled: bool = True,
        transform: ty.Optional[ty.List[transform_func_signature]] = None
    ) -> None:
        self.df = df
        self.root_dir = root_dir
        self.transform = transform
        self.labeled = labeled

    def __len__(self):
        return self.df.shape[0]

    def __getitem__(self, idx: int) -> transform_func_signature:
        if torch.is_tensor(idx):
            idx = idx.tolist()

        # Read image
        img_name = os.path.join(self.root_dir, self.df.filename.iloc[idx])
        image = io.imread(img_name)

        #print(f"Dimensiones originales de la imagen: {image.shape}")  # Agregar para depuración

        if image.ndim == 2:  # Si la imagen está en escala de grises
            image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)  # Convertir a RGB
        elif image.shape[2] == 4:  # Si la imagen es RGBA
            image = image[:, :, :3]

        sample = {'image': image}

        if self.labeled:
            # Read labels
            img_class = self.df.class_id.iloc[idx]
            #name_class = self.df['class'].iloc[idx]
            img_bbox = self.df.iloc[idx, 1:5]
            #img_h = self.df.h.iloc[idx]
            #img_w = self.df.w.iloc[idx]

            img_bbox = np.array([img_bbox]).astype('float')
            img_class = np.array([img_class]).astype('int')
            #img_h = np.array([img_h]).astype('int')
            #img_w = np.array([img_w]).astype('int')
            sample.update({'bbox': img_bbox, 'class_id': img_class})#,'name_class':name_class,'w': img_w, 'h': img_h

        if self.transform:
            sample = self.transform(sample)

        return sample

def draw_bbox(img, bbox, color):
    xmin, ymin, xmax, ymax = bbox
    img = cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color, 2)
    return img

def normalize_bbox(bbox, factor : int = 416):
    return list(map(lambda x: int(x * factor), bbox))

def draw_bboxes(imgs, bboxes, colors):
    for i, (img, bbox, color) in enumerate(zip(imgs, bboxes, colors)):
        imgs[i] = draw_bbox(img, bbox, color)
    return imgs

def draw_classes(imgs, classes, colors, origin, offset: int = 5, prefix: str = ''):
    for i, (img, class_id, color) in enumerate(zip(imgs, classes, colors)):
        if type(c) == list:
            name_class_ = id2obj[classes[i]]
        else:
            name_class_ = id2obj[classes[i][0]]

        # Adjust the Y-coordinate of the origin
        x, y = origin
        y = max(18, y + offset)  # Ensure the text is within the image bounds

        imgs[i] = cv2.putText(
            img, f'{prefix}{name_class_}',  # class_id.squeeze()
            (x, y), cv2.FONT_HERSHEY_SIMPLEX,
            0.8,  # Font scale (adjust as needed)
            color, 2,  # Thickness
            cv2.LINE_AA
        )
    return imgs

def draw_predictions(imgs, classes, bboxes, colors, origin):
    assert all(len(x) > 0 for x in [imgs, classes, bboxes, colors])
    if len(colors) == 1:
        colors = [colors[0] for _ in imgs]
    imgs = draw_bboxes(imgs, bboxes, colors)
    imgs = draw_classes(imgs, classes, colors, origin)
    return imgs

train_root_dir = osp.join(DATA_DIR, "resized")#, "train"
train_ds = PristimantisDataset(train_df, root_dir=train_root_dir)

num_imgs = 6
start_idx = 0

samples = [train_ds[i] for i in range(start_idx, num_imgs)]

imgs = [s['image'] for s in samples]
bboxes = [normalize_bbox(s['bbox'].squeeze()) for s in samples]
classes = [s['class_id'] for s in samples]

imgs = draw_predictions(imgs, classes, bboxes, [(0, 150, 0)], (5, 10))#(150, 10)

fig = plt.figure(figsize=(30, num_imgs))

for i, img in enumerate(imgs):
    fig.add_subplot(1, num_imgs, i+1)
    plt.imshow(img)

plt.show()

train_ds = PristimantisDataset(train_df, root_dir=train_root_dir)

means = np.zeros(3)
stds = np.zeros(3)
n_images = 0

for x in train_ds:
    img = x['image']#astype(np.float32)  # Asegúrate de que la imagen está en float para cálculos precisos
    n_images += 1

    for channel in range(3):
        channel_pixels = img[..., channel]
        # Acumular la suma y suma de cuadrados para calcular la media y desviación estándar
        means[channel] += np.mean(channel_pixels)
        stds[channel] += np.std(channel_pixels)

# Calcular la media y desviación estándar final
means /= n_images
stds /= n_images

print(means)
print(stds)

import numpy as np
import torch
import cv2
import albumentations as A # Make sure A is imported

class ToTensor(object):
    """Convert ndarrays in sample to Tensors."""

    def __call__(self, sample):
        image = sample['image']

        # swap color axis because
        # numpy image: H x W x C (0,1,2)
        # torch image: C x H x W
        image = image.transpose((2, 0, 1))
        image = torch.from_numpy(image).float()
        sample.update({'image': image})
        return sample

class Normalizer(object):

    def __init__(self, stds, means):
        """
        Arguments:

            stds: array of length 3 containing the standard deviation of each channel in RGB order.
            means: array of length 3 containing the means of each channel in RGB order.
        """
        self.stds = stds
        self.means = means

    def __call__(self, sample):
        """
        Sample: a dicitonary containing:
            image: sample image in format (C, H, W)
        Returns:
            the image in (C, H, W) format with the channels normalized.
        """
        image = sample['image']

        for channel in range(3):
            image[channel] = (image[channel] - means[channel]) / stds[channel]

        sample['image'] = image
        return sample

class TVTransformWrapper(object):
    """Torch Vision Transform Wrapper
    """
    def __init__(self, transform: torch.nn.Module):
        self.transform = transform

    def __call__(self, sample):
        sample['image'] = self.transform(sample['image'])
        return sample




class AlbumentationsWrapper(object):

    def __init__(self, transform):
        self.transform = transform

    def __call__(self, sample):

        bboxes_list = [sample['bbox'].squeeze().tolist()] if sample['bbox'].ndim > 1 else [sample['bbox'].tolist()]

        # Ensure category_ids is a list corresponding to bboxes
        # Handle cases where class_id might be a single number or a numpy array
        category_ids_list = [sample['class_id'].squeeze().tolist()] if isinstance(sample['class_id'], np.ndarray) else [sample['class_id']]



        image_to_albu = sample['image'].astype(np.uint8)


        transformed = self.transform(
            image=image_to_albu,
            bboxes=bboxes_list,
            category_ids=category_ids_list
        )


        sample['image'] = transformed['image']


        if transformed['bboxes']:

            transformed_bbox = np.array(transformed['bboxes'][0]).reshape(1, 4)


            clipped_bbox = np.clip(transformed_bbox, 0.0, 1.0 - 1e-6)


            min_size_threshold = 0.001 # Define a minimum size threshold for the bbox

            if (clipped_bbox[0, 2] <= clipped_bbox[0, 0] + min_size_threshold) or \
               (clipped_bbox[0, 3] <= clipped_bbox[0, 1] + min_size_threshold):

                 print(f"Warning: Bounding box became invalid or too small after transformation. Assigning placeholder.")
                 sample['bbox'] = np.array([[0.0, 0.0, 0.01, 0.01]])


            else:

                 sample['bbox'] = clipped_bbox
        else:
             # Handle the case where Albumentations removed the bbox entirely (e.g., it went outside image bounds)
             print(f"Warning: Bounding box removed by transformation. Assigning placeholder.")
             sample['bbox'] = np.array([[0.0, 0.0, 0.01, 0.01]]) # Assign a tiny valid bbox as a placeholder



        if 'category_ids' in transformed and transformed['category_ids']:

             if transformed['bboxes']: # Only update class_id if Albumentations returned at least one bbox
                 sample['class_id'] = np.array([transformed['category_ids'][0]])
        else:
             print("Warning: Category ID lost for a sample from Albumentations output.")

             if np.array_equal(sample['bbox'], np.array([[0.0, 0.0, 0.01, 0.01]])):
                 pass
             else:
                 print("Warning: Category ID lost, but a valid bbox seems to exist.")
                 pass


        return sample

"""# Transformaciones"""

common_transforms = [
    ToTensor(),
    Normalizer(
        means=means,
        stds=stds,
    )
]

train_data_augmentations = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.1, p=0.5),
    A.ColorJitter(brightness=0.1, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),
    A.CLAHE(p=0.3),
    A.Rotate(limit=(-20, 20), p=0.5),
    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=60, p=0.5, border_mode=cv2.BORDER_CONSTANT),
    A.Resize(height=416, width=416, p=1),
    A.Perspective(scale=(0.05, 0.1), p=0.5),

    ],
    bbox_params=A.BboxParams(
        format='albumentations',
        label_fields=['category_ids'],

    )
)

train_transforms = torchvision.transforms.Compose(
    [
        # Apply Albumentations first using the corrected wrapper
        AlbumentationsWrapper(train_data_augmentations),
        # Then apply common transforms like ToTensor and Normalizer
    ] + common_transforms # Common transforms are applied after AlbumentationsWrapper
)

eval_transforms = torchvision.transforms.Compose(common_transforms)

train_ds = PristimantisDataset(train_df, root_dir=train_root_dir)

x = next(iter(train_ds))
x_transformed = copy.deepcopy(x)
x_transformed = train_transforms(x_transformed)

bbox = x_transformed['bbox'].squeeze()

if not np.all((bbox >= 0.0) & (bbox <= 1.0)):
    print("⚠️ Bbox fuera de rango detectado y eliminado.")
    # Puedes omitir esta muestra o eliminarla del DataFrame original si lo deseas
    train_df = train_df.drop(index=0).reset_index(drop=True)
else:
    # Solo si es válido, continúa con visualización
    original_img = x['image']
    transformed_img = x_transformed['image'].numpy().transpose(1, 2, 0)

    original_img = draw_bbox(original_img, normalize_bbox(x['bbox'].squeeze()), (0, 255, 0))
    transformed_img = draw_bbox(transformed_img, normalize_bbox(bbox), (0, 255, 0))

    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))
    axes[0].imshow(original_img)
    axes[0].set_title('Original digit')
    axes[1].imshow(transformed_img)
    axes[1].set_title('Transformed digit')
    plt.show()

def bbox_invalid(row):
    bbox = [row['xmin'], row['ymin'], row['xmax'], row['ymax']]
    # Revisa si hay valores inválidos
    if any(pd.isnull(bbox)):
        return True
    if bbox[2] <= bbox[0] or bbox[3] <= bbox[1]:  # x_max <= x_min o y_max <= y_min
        return True
    return False

# Ver registros con problemas
invalid_rows = train_df[train_df.apply(bbox_invalid, axis=1)]

print(f"{len(invalid_rows)} bbox(es) inválido(s) encontrados")
display(invalid_rows[['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'class_id']])

for i in range(len(train_ds)):
    img = train_ds[i]['image']
    print(f"Image {i}: {img.shape}")

train_ds = PristimantisDataset(df, root_dir=train_root_dir, transform=train_transforms)
train_data = torch.utils.data.DataLoader(train_ds, batch_size=16)

for x in train_data:
    print(x['image'].size())
    break

train_ds.df

x_transformed

"""# Evaluation metrics

Here we define IoU for localization task, accuracy for classification task and loss
"""

def iou(y_true: Tensor, y_pred: Tensor):
    # Ensure y_true is at least 2D (N, 4)
    y_true_squeezed = y_true.squeeze()
    if y_true_squeezed.ndim == 1:
        y_true_processed = y_true_squeezed.unsqueeze(0)
    else:
        y_true_processed = y_true_squeezed

    # Ensure y_pred is at least 2D (M, 4)
    y_pred_squeezed = y_pred.squeeze()
    if y_pred_squeezed.ndim == 1:
        y_pred_processed = y_pred_squeezed.unsqueeze(0)
    else:
        y_pred_processed = y_pred_squeezed

    pairwise_iou = torchvision.ops.box_iou(y_true_processed, y_pred_processed)

    result = torch.trace(pairwise_iou) / pairwise_iou.size()[0]
    return result

def accuracy(y_true: Tensor, y_pred: Tensor):
    pred = torch.argmax(y_pred, axis=-1)
    y_true = y_true.squeeze()
    correct = torch.eq(pred, y_true).float()
    total = torch.ones_like(correct)
    result = torch.divide(torch.sum(correct), torch.sum(total))
    return result

def loss_fn(y_true, y_preds, alpha: float = 0.4):
    cls_y_true, cls_y_pred = y_true['class_id'].long(), y_preds['class_id'].float().unsqueeze(-1)
    reg_y_true, reg_y_pred = y_true['bbox'].float().squeeze(), y_preds['bbox'].float().squeeze()

    cls_loss = F.cross_entropy(cls_y_pred, cls_y_true)

    reg_loss = F.mse_loss(reg_y_pred, reg_y_true)
    # Adds weights to both tasks
    total_loss = (1 - alpha) * cls_loss + alpha * reg_loss
    return dict(loss=total_loss, reg_loss=reg_loss,cls_loss=cls_loss)

"""# Transfer learning

Here we implemetn ResNet 50 Pre-Trained model
"""

import torch
print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))

from torchvision.models import resnet50
import torch.nn as nn
import torch

class FeatureExtractor(nn.Module):
    def __init__(self, model):
        super(FeatureExtractor, self).__init__()
        # Everything except the final layer
        self.features = nn.Sequential(*list(model.children())[:-2])
        self.pooling = nn.AdaptiveAvgPool2d((1, 1)) # Capa especial de agrupamiento
        self.flatten = nn.Flatten()
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        out = self.features(x)
        out = self.pooling(out) # Aplicación de la capa de agrupamiento aquí
        out = self.flatten(out)
        out = self.dropout(out)
        return out

# Load the ResNet50 model on CPU first
from torchvision.models import resnet50, ResNet50_Weights
resnet50_model = resnet50(weights=ResNet50_Weights.DEFAULT)

resnet50_model = resnet50_model.to(device)

summary(resnet50_model, (3, 416, 416))

samples[2]['image']

"""# Model"""

def get_output_shape(model: nn.Sequential, image_dim: ty.Tuple[int, int, int]):
    return model(torch.rand(*(image_dim)).to(device)).data.shape

class Model(nn.Module):
    def __init__(self, input_shape: ty.Tuple[int, int, int] = (3, 416, 416), n_classes: int = 12):

        super().__init__()

        self.input_shape = input_shape

        # When doing transfer learning, use pretrained model instead of custom backbone
        self.backbone = resnet50_model

        backbone_output_shape = get_output_shape(self.backbone, [1, *input_shape])
        backbone_output_features = reduce(lambda x, y: x*y, backbone_output_shape)

        self.cls_head = nn.Sequential(
            nn.Linear(backbone_output_features, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, n_classes)
        )
        self.reg_head = nn.Sequential(
            nn.Linear(in_features=backbone_output_features, out_features=1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(1024, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
             nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 4)
        )

    def forward(self, x: Tensor) -> ty.Dict[str, Tensor]:
        features = self.backbone(x)
        cls_logits = self.cls_head(features)
        cls_probs = F.softmax(cls_logits, dim=1)
        pred_bbox = self.reg_head(features)
        predictions = {'bbox': pred_bbox, 'class_id': cls_logits, 'class_probs': cls_probs}
        return predictions

print('image', x['image'].size())
model = Model(input_shape=(3, 416, 416), n_classes=12).to(device)
x['image'] = x['image'].to(device)
preds = model(x['image'])
preds

def evaluate(
    logs: ty.Dict[str, ty.Any],
    labels: ty.Dict[str, Tensor],
    preds: ty.Dict[str, Tensor],
    eval_set: str,
    metrics: ty.Dict[str, ty.Callable[[Tensor, Tensor], Tensor]],
    losses: ty.Optional[ty.Dict[str, Tensor]] = None,
) -> ty.Dict[str, ty.Any]:

    if losses is not None:
        for loss_name, loss_value in losses.items():
            logs[f'{eval_set}_{loss_name}'] = loss_value

    for task_name, label in labels.items():
        for metric_name, metric in metrics[task_name]:
            value = metric(label, preds[task_name])
            logs[f'{eval_set}_{metric_name}'] = value

    return logs

def step(
    model: Model,
    optimizer: Optimizer,
    batch: PristimantisDataset,
    loss_fn: ty.Callable[[ty.Dict[str, torch.Tensor]], torch.Tensor],
    device: str,
    train: bool = False,
) -> ty.Tuple[ty.Dict[str, Tensor], ty.Dict[str, Tensor]]:

    if train:
        optimizer.zero_grad()

    #img = batch['image'].to(device)
    img = batch.pop('image').to(device)

    for k in list(batch.keys()):
        batch[k] = batch[k].to(device)

    preds = model(img.float())
    losses = loss_fn(batch, preds)
    final_loss = losses['loss']

    if train:
        final_loss.backward()
        optimizer.step()

    return losses, preds


def train(
    model: Model,
    optimizer: Optimizer,
    dataset: DataLoader,
    eval_datasets: ty.List[ty.Tuple[str, DataLoader]],
    loss_fn: ty.Callable[[ty.Dict[str, torch.Tensor]], torch.Tensor],
    metrics: ty.Dict[str, ty.Callable[[Tensor, Tensor], Tensor]],
    callbacks: ty.List[ty.Callable[[ty.Dict[ty.Any, ty.Any]], None]],
    device: str,
    train_steps: 800,
    eval_steps: 20,
) -> Model:
    # Send model to device (GPU or CPU)
    model = model.to(device)
    iters = 0
    iterator = iter(dataset)
    assert train_steps > eval_steps, 'Train steps should be greater than the eval steps'


    # Early stopping setup
    best_val_loss = float('inf')
    patience = 10  # Number of eval steps to tolerate no improvement
    patience_counter = 0
    early_stopped = False

    while iters <= train_steps:
        logs = dict()
        logs['iters'] = iters
        try:
            batch = next(iterator)
        except StopIteration:
            iterator = iter(dataset)
            batch = next(iterator)
        # Send batch to device
        losses, preds = step(model, optimizer, batch, loss_fn, device, train=True)
        logs = evaluate(logs, batch, preds, 'train', metrics, losses)

        # Eval every eval_steps iterations
        if iters % eval_steps == 0:
            # Evaluate
            # Deactives layers that only needed to train
            # https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615
            model.eval()

            # Avoids calculating gradients in evaluation dataset.
            with torch.no_grad():

                for name, dataset in eval_datasets:

                    for batch in dataset:
                        losses, preds = step(model, optimizer, batch, loss_fn, device, train=False)
                        logs = evaluate(logs, batch, preds, name, metrics, losses)

                     # Early stopping check after all batches in the current dataset are processed
                    current_val_loss = logs[f'{name}_loss']  # Assuming 'loss' is being tracked
                    if current_val_loss < best_val_loss:
                        best_val_loss = current_val_loss
                        patience_counter = 0
                    else:
                        patience_counter += 1
                    if patience_counter >= patience:
                        print("Early stopping triggered due to no improvement in validation loss.")
                        early_stopped = True
                        break

            if early_stopped:
                print("Exiting training due to early stopping.")
                break


        for callback in callbacks:
            callback(logs)

        iters += 1

    return model

batch_size = 16
lr = 0.00005

# Data
train_ds = PristimantisDataset(train_df, root_dir=train_root_dir, transform=train_transforms)
val_ds = PristimantisDataset(val_df, root_dir=train_root_dir, transform=eval_transforms)

train_data = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)
val_data = DataLoader(val_ds, batch_size=batch_size, num_workers=0)

# Model
model = Model().to(device)
summary(model, model.input_shape)

def printer(logs: ty.Dict[str, ty.Any], history: ty.Dict[str, list], best_metrics: dict, model: nn.Module, filename='transfer_Pristimantis25Mayo6.pth'):
    if logs['iters'] % 10 != 0:
        return

    print(f'Iteration #: {logs["iters"]}')
    val_iou = None
    val_accuracy = None

    for name, value in logs.items():
        if name == 'iters':
            continue

        if isinstance(value, torch.Tensor):
            value = value.item()

        if name not in history:
            history[name] = []
        history[name].append(value)

        print(f'\t{name} = {value}')

        if name == 'val_iou':
            val_iou = value
        elif name == 'val_accuracy':
            val_accuracy = value

    # Calcular el score compuesto si ambos están disponibles
    # Corrected indentation for this block
    if val_iou is not None and val_accuracy is not None:
        score = 0.6 * val_accuracy + 0.4 * val_iou
        print(f'\t→ Score compuesto (60% accuracy, 40% IoU): {score:.4f}')

        if score > best_metrics.get('composite_score', float('-inf')):
            best_metrics['composite_score'] = score
            torch.save(model.state_dict(), filename)
            print(f'Model improved composite score and saved to {filename}')

    if 'early_stopped' in logs and logs['early_stopped']:
        print('Early stopping has been triggered. Exiting training loop.')

    print()

"""We run the model and save the best Iteration"""

best_metrics = {}

history = {}

from torch.utils.data import DataLoader

train_loader = DataLoader(
    train_df,
    batch_size=16,
    shuffle=True,
    collate_fn=lambda x: tuple(zip(*x))  # necesario para detección con múltiples bboxes
)

# Optimizer
optimizer = torch.optim.Adam(lr=lr, params=model.parameters(), weight_decay=0.01)


model = train(
    model,
    optimizer,
    train_data,
    eval_datasets=[('val', val_data)],
    loss_fn=loss_fn,
    metrics={
        'bbox': [('iou', iou)],
        'class_id': [('accuracy', accuracy)]
    },
    callbacks=[lambda logs: printer(logs, history, best_metrics, model,'/content/transfer_Pristimantis25Mayo6.pth')],
    device=device,
    train_steps=800,
    eval_steps=10
)

import matplotlib.pyplot as plt


def plot_metrics(history):
    fig, axs = plt.subplots(3, 1, figsize=(10, 15))

    # Plotting Training and Validation Loss
    train_loss = history.get('train_loss', [])
    val_loss = history.get('val_loss', [])
    axs[0].plot(train_loss, label='Training Loss', color='red')
    axs[0].plot(val_loss, label='Validation Loss', color='blue')
    axs[0].set_title('Training & Validation Loss')
    axs[0].set_xlabel('Iteration')
    axs[0].set_ylabel('Loss')
    axs[0].legend()

    # Plotting IoU
    train_iou = history.get('train_iou', [])
    val_iou = history.get('val_iou', [])
    axs[1].plot(train_iou, label='Training IoU', color='red')
    axs[1].plot(val_iou, label='Validation IoU', color='blue')
    axs[1].set_title('Training & Validation IoU')
    axs[1].set_xlabel('Iteration')
    axs[1].set_ylabel('IoU')
    axs[1].legend()

    # Plotting Accuracy
    train_accuracy = history.get('train_accuracy', [])
    val_accuracy = history.get('val_accuracy', [])
    axs[2].plot(train_accuracy, label='Training Accuracy', color='red')
    axs[2].plot(val_accuracy, label='Validation Accuracy', color='blue')
    axs[2].set_title('Training & Validation Accuracy')
    axs[2].set_xlabel('Iteration')
    axs[2].set_ylabel('Accuracy')
    axs[2].legend()

    plt.tight_layout()
    plt.show()

# Call to plot the metrics after training
plot_metrics(history)

print('image', x['image'].size())
model = Model(input_shape=(3, 416, 416), n_classes=12).to(device)
x['image'] = x['image'].to(device)
preds = model(x['image'])
preds

torch.cuda.empty_cache()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""# Load Model"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = Model()
model.load_state_dict(torch.load('transfer_Pristimantis25Mayo6.pth', map_location=device))
model = model.to(device)
model.eval()

# Ensure the model is in evaluation mode
model.eval()
num_imgs = 16
ncols = 5
nrows = math.ceil(num_imgs / ncols)

start_idx = 0

inference_ds = PristimantisDataset(val_df.iloc[start_idx:start_idx+num_imgs], root_dir=train_root_dir)
inference_data = DataLoader(inference_ds, batch_size=num_imgs, num_workers=0, shuffle=False)
inference_batch = next(iter(inference_data))
inference_imgs = np.empty((num_imgs, 3, 416, 416))

transform = eval_transforms

for i, img in enumerate(inference_batch['image']):
    inference_imgs[i] = transform(dict(image=img.numpy()))['image'].numpy()

preds = model(torch.tensor(inference_imgs).float().to(device))

samples = [inference_ds[i] for i in range(start_idx, num_imgs)]

imgs = [s['image'] for s in samples]
bboxes = [normalize_bbox(s['bbox'].squeeze()) for s in samples]
classes = [s['class_id'] for s in samples]

pred_bboxes = preds['bbox'].detach().cpu().numpy()
pred_bboxes = [normalize_bbox(bbox) for bbox in pred_bboxes]
pred_classes = preds['class_id'].argmax(-1).detach().cpu().numpy()

model.load_state_dict(torch.load('/content/transfer_Pristimantis25Mayo6.pth'))
model.to(device)

preds['class_probs']

imgs_draw = [img.copy() for img in imgs]

idx_to_name = {}


for pred, name in zip(pred_classes, classes):
    if isinstance(pred, int):
        idx_to_name[pred] = str(name)

class_id_map = train_df[['class_id', 'class']].drop_duplicates().sort_values('class_id')

print("Relación actual class_id → nombre de clase:")
print(class_id_map.to_string(index=False))

class_names = [
    'Pristimantis achatinus',
    'Pristimantis brevifrons',
    'Pristimantis erythropleura',
    'Pristimantis palmeri',
    'Pristimantis ridens',
    'Pristimantis gaigei',
    'Pristimantis w-nigrum',
    'Pristimantis thectopternus',
    'Pristimantis latidiscus',
    'Pristimantis buckleyi',
    'Pristimantis orpacobates',
    'Pristimantis boulengeri',
]

import cv2
import matplotlib.pyplot as plt

# Hacemos una copia limpia de las imágenes originales
imgs_draw = [img.copy() for img in imgs]

# Dibujo en verde: clase real
imgs_draw = draw_predictions(imgs_draw, classes, bboxes, [(0, 150, 0)], (5, 10))

# Dibujo en rojo: clase predicha (top-1)
pred_classes_ = [np.array([pred_classes[i]]) for i in range(num_imgs)]
imgs_draw = draw_predictions(imgs_draw, pred_classes_, pred_bboxes, [(200, 0, 0)], (5, 80))

for i in range(num_imgs):
    img = imgs_draw[i]
    top_probs, top_idxs = torch.topk(preds['class_probs'][i], k=3)

    # Crear el texto Top-3 con nombre
    top_text = [
        f"{class_names[top_idxs[j].item()]}: {top_probs[j] * 100:.1f}%"
        for j in range(3)
    ]

    # Determinar posición del recuadro (esquina inferior derecha)
    h, w, _ = img.shape
    box_x, box_y = w - 310, h - 110  # ajustar si el texto se sale
    box_w, box_h = 300, 100

    # Dibujar fondo blanco del recuadro
    img = cv2.rectangle(img, (box_x, box_y), (box_x + box_w, box_y + box_h), (255, 255, 255), -1)

    # Dibujar borde del recuadro
    img = cv2.rectangle(img, (box_x, box_y), (box_x + box_w, box_y + box_h), (0, 0, 0), 2)

    # Escribir cada predicción en el recuadro
    for j, text in enumerate(top_text):
        y_offset = box_y + 25 + j * 25
        img = cv2.putText(
            img,
            text,
            (box_x + 10, y_offset),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.6,
            (0, 0, 255),
            2,
            cv2.LINE_AA
        )

    imgs_draw[i] = img

# Mostrar resultado final
fig, axes = plt.subplots(nrows=nrows, ncols=num_imgs // nrows, figsize=(60, 60))
for i, ax in enumerate(axes.flat):
    ax.imshow(imgs_draw[i])
    ax.axis('off')
plt.tight_layout()
plt.show()

def draw_predictions(images, classes, bboxes, colors, offsets, thickness=4):
    for i, img in enumerate(images):
        # Ensure the bbox element is a numpy array before calling astype
        bbox_array = np.array(bboxes[i])
        x1, y1, x2, y2 = bbox_array.astype(int)
        color = colors[0]
        offset_x, offset_y = offsets
        cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)
        # Elimina cualquier texto aquí para evitar duplicación
    return images

import cv2
import matplotlib.pyplot as plt
import numpy as np
import torch

# Hacemos una copia limpia de las imágenes originales
imgs_draw = [img.copy() for img in imgs]

# Dibujo de bounding boxes (más gruesos, sin texto)
imgs_draw = draw_predictions(imgs_draw, classes, bboxes, [(0, 150, 0)], (5, 10), thickness=4)
pred_classes_ = [np.array([pred_classes[i]]) for i in range(num_imgs)]
imgs_draw = draw_predictions(imgs_draw, pred_classes_, pred_bboxes, [(200, 0, 0)], (5, 80), thickness=4)

for i in range(num_imgs):
    img = imgs_draw[i]
    top_probs, top_idxs = torch.topk(preds['class_probs'][i], k=3)

    h, w, _ = img.shape
    font_scale = 0.75
    font_thickness = 2

    # Textos
    real_class_id = classes[i].item()
    real_text = f"True: {class_names[real_class_id]}"
    pred_class_id = pred_classes[i]
    pred_text = f"Pred: {class_names[pred_class_id]}"

    # Calcular tamaño del texto para cuadros ajustados
    (tw_real, th), _ = cv2.getTextSize(real_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)
    (tw_pred, _), _ = cv2.getTextSize(pred_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)

    # Posiciones
    x_start = 10
    y_start_real = 10
    y_start_pred = 50
    padding = 10

    # Cuadro para True
    cv2.rectangle(img, (x_start, y_start_real),
                  (x_start + tw_real + 2 * padding, y_start_real + th + padding),
                  (255, 255, 255), -1)
    cv2.rectangle(img, (x_start, y_start_real),
                  (x_start + tw_real + 2 * padding, y_start_real + th + padding),
                  (0, 0, 0), 1)
    cv2.putText(img, real_text, (x_start + padding, y_start_real + th),
                cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 150, 0), font_thickness, cv2.LINE_AA)

    # Cuadro para Pred
    cv2.rectangle(img, (x_start, y_start_pred),
                  (x_start + tw_pred + 2 * padding, y_start_pred + th + padding),
                  (255, 255, 255), -1)
    cv2.rectangle(img, (x_start, y_start_pred),
                  (x_start + tw_pred + 2 * padding, y_start_pred + th + padding),
                  (0, 0, 0), 1)
    cv2.putText(img, pred_text, (x_start + padding, y_start_pred + th),
                cv2.FONT_HERSHEY_SIMPLEX, font_scale, (200, 0, 0), font_thickness, cv2.LINE_AA)

    # Cuadro inferior izquierdo con top-3
    box_x, box_y = 20, h - 120  # Más a la izquierda
    box_w, box_h = 383, 123  # Más ancho
    cv2.rectangle(img, (box_x, box_y), (box_x + box_w, box_y + box_h), (255, 255, 255), -1)
    cv2.rectangle(img, (box_x, box_y), (box_x + box_w, box_y + box_h), (0, 0, 0), 2)

    top_text = [
        f"{class_names[top_idxs[j].item()]}: {top_probs[j] * 100:.1f}%"
        for j in range(3)
    ]

    for j, text in enumerate(top_text):
        y_offset = box_y + 30 + j * 30
        cv2.putText(img, text, (box_x + 15, y_offset),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2, cv2.LINE_AA)

    # Redimensionar imagen al 150%
    img = cv2.resize(img, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_CUBIC)
    imgs_draw[i] = img

# Mostrar resultado final
fig, axes = plt.subplots(nrows=nrows, ncols=num_imgs // nrows, figsize=(60, 60))
for i, ax in enumerate(axes.flat):
    ax.imshow(imgs_draw[i])
    ax.axis('off')
plt.tight_layout()
plt.show()

"""# Confussion Matrix"""

val_ds = PristimantisDataset(val_df, root_dir=train_root_dir, transform=eval_transforms)
val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2)

true_labels = []
pred_labels = []

model.eval()
with torch.no_grad():
    for batch in val_loader:
        images = batch['image'].to(device)
        labels = batch['class_id'].to(device)

        outputs = model(images)
        preds = torch.argmax(outputs['class_probs'], dim=1)

        true_labels.extend(labels.cpu().numpy())
        pred_labels.extend(preds.cpu().numpy())

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(true_labels, pred_labels)

fig, ax = plt.subplots(figsize=(12, 12))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)

# Create the plot and increase font sizes
disp.plot(
    include_values=True,
    cmap='Blues',
    ax=ax,
    xticks_rotation='vertical',
)

# Increase font sizes for better readability
ax.set_title("Confusion Matrix - Validation Data", fontsize=18, weight='bold')
ax.set_xlabel("Predicted label", fontsize=16)
ax.set_ylabel("True label", fontsize=16)
ax.tick_params(axis='x', labelsize=14)
ax.tick_params(axis='y', labelsize=14)

plt.tight_layout()
plt.show()

# Supone que ya tienes estas dos variables:
# cm: matriz de confusión (como array de numpy)
# class_names: lista de nombres de clase en orden de class_id

# Crear DataFrame
cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)

# Mostrar como tabla
cm_df

cm_df.to_csv("/content/confusion_matrix.csv")

"""# Mean Per-Class Accuracy"""

# Crear DataFrame de resumen por especie
species_stats = []

for idx, species in enumerate(class_names):
    total_images = np.sum(cm[idx])               # Total de imágenes reales de esta clase
    correct = cm[idx, idx]                       # Clasificadas correctamente
    accuracy = (correct / total_images) * 100 if total_images > 0 else 0

    species_stats.append({
        "Especie": species,
        "Total de imágenes": total_images,
        "Clasificadas correctamente": correct,
        "Accuracy (%)": round(accuracy, 2)
    })

# Convertir a DataFrame
stats_df = pd.DataFrame(species_stats)

# Mostrar el DataFrame
import os
stats_df_path = "/content/classification_accuracy_per_species.csv"
stats_df.to_csv(stats_df_path, index=False)
print(f"Archivo guardado en: {stats_df_path}")

stats_df